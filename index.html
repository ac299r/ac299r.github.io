<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>AC299r</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body>

		<!-- Header -->
			<header id="header">
				<h1><strong><a href="index.html">AC299r</h1>
				<nav id="nav">
					<ul>
						<li><a href="#main">Home</a></li>
						<li><a href="#multi-sal">Multi-duration Saliency</a></li>
						<li><a href="#attent">Attention Methodologies</a></li>
					</ul>
				</nav>
			</header>

			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

		<!-- Main -->
			<section id="main" class="wrapper">
				<div class="container">

					<header class="major special">
						<h2>AC299r - Independent Research</h2>
						<p>Harvard University</p>
					</header>
					<div class="flex-container">
						<div class="imgdiv">
							<div class="img" style="background-image:url('images/people/Pat.jpg');"></div>
							<p>Pat Sukhum</p>
						</div>
						<div class="imgdiv">
							<div class="img" style="background-image:url('images/people/Matteo.jpg');"></div>
							<p>Yun Bin (Matteo) Zhang</p>
						</div>
					</div>
					<p>During this semester, we worked under Zoya Bylinskii on multi-duration saliency and attention methodologies projects. We also worked in conjunction with other student researchers.  For multi-duration saliency, our group developed a deep learning network that can predict saliency at different viewing durations. For attention methodologies, our group shows comparison between 4 different user interfaces for collecting crowdsourced annotations of the important regions in images. Our work results in two paper submissions currently under review.  </p>


				</div>
			</section>

			<section id="multi-sal" class="wrapper">
				<div class="container">

					<h2>Multi-duration Saliency</h2>
					<img src="images/iccv.png" alt="" style="width:50%"/>
					<p>What jumps out at you in a single glance of an image is different than what you might notice after closer inspection. Despite this, models of visual saliency have ignored the temporal aspect of visual attention and have produced prediction maps at fixed viewing durations. As a result, current applications leveraging saliency models are rigidly tailored for a fixed viewing duration, depending on the attention dataset they were trained on. To incorporate knowledge of viewing duration into saliency modeling, we first develop a â€œCodecharts UI" and use it to crowdsource human attention at various viewing durations. We collect the CodeCharts1K dataset, which contains viewing data at 0.5, 3, and 5 seconds on 1000 images from diverse computer vision datasets. Our analysis shows distinct differences in gaze locations at these time points and exposes recurring temporal patterns about which objects attract attention.</p>
 					<p>Using insights from this analysis, we develop an LSTM-based model of saliency that simultaneously trains on data from multiple time points. Our Multi-Duration Saliency Excited Model (MD-SEM) achieves state-of-the-art performance on the LSUN 2017 Challenge with 57% fewer parameters than comparable architectures. Additionally, it provides new predictions at time points that current models can not predict. We explore applications where multi-duration saliency can be used to prioritize the visual content to keep, transmit, and render.</p>

					<a href="http://iccvpat.github.io">Website</a></br>
					<a href="papers/iccv.pdf">Paper</a>



				</div>
			</section>

			<section id="attent" class="wrapper">
				<div class="container">

					<h2>Attention Methodologies</h2>
					<img src="images/attent.png" alt="" style="width:50%"/>
					<p>In this work, we compare and analyze four UIs for collecting crowdsourced attention maps. We introduce ZoomMaps, which allows collecting attention data at multiple image scales using the familiar zoom gesture on a mobile phone. We conduct the first in-depth analysis of CodeCharts, an interface for approximating eye locations at precise viewing durations. These novel interfaces are compared to ImportAnnots, a tool for collecting annotations of graphic design importance, and the moving-window interface BubbleView. We lay out in detail the methodology used for each of these interfaces and discuss which types of images and potential applications are appropriate for each. Additionally, we collect CodeCharts, ZoomMaps, and ImportAnnots data on five types of image stimuli--natural images, resumes, graphic designs, infographics, and academic posters--and analyze the results to characterize in detail what can be learned from each method of data collection.</p>
					<a href="http://turkeyes.mit.edu">Website</a></br>
					<a href="papers/attent.pdf">Paper</a>

				</div>
			</section



		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
